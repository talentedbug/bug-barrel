---
date: '2024-12-22T22:19:21+08:00'
draft: false
title: 'AI 融合开发的爬虫实践'
typora-copy-images-to: '../../static/img/${filename}.d'
---

~~本蛾子可是**蛾子**，最擅长的就是爬虫啦~~ 😄

其实主要是本蛾子最近看到 GitHub Copilot 竟然有了免费计划（不知道是不是被 CoCopilot 逼的），又想起来今年都 2024 年了，不会融合 AI 开发的程序员快被时代淘汰了。于是本蛾子就试着拿 Microsoft Copilot，在尽可能少的人工干预下，写出的代码可用性究竟如何。

为什么用 Microsoft Copilot？这是本蛾子已知的 GPT-4o 模型限制最少的，上下文、对话数量、文生图都没有硬限制。

> 本文中生成的所有代码都存于 [talentedbug/bugscrawler](https://github.com/talentedbug/bugscrawler) 中，使用注意版权风险。

## 🏹 快手

网上有很多快手小姐姐短视频的 API，都是开放的，主要是由于快手因为不明原因，并未对其视频设置反盗链，只要能获得链接列表就能下载，而这并不是很难。

这个挑战应该是最小的，因为可以直接获得链接，下载即可。

```python
import requests
import os

headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/102.0.0.0 Safari/537.36'
}

def download_file(url, directory, file_number):
    try:
        if not os.path.exists(directory):
            os.makedirs(directory)
        
        file_name = os.path.join(directory, f"{file_number}.mp4")
        with requests.get(url, stream=True, headers=headers) as r:
            r.raise_for_status()
            with open(file_name, 'wb') as f:
                for chunk in r.iter_content(chunk_size=8192):
                    f.write(chunk)
        print(f"[Info] Downloaded: {file_name}")
    except requests.RequestException as e:
        print(f"[Warning] Download error: {e}")

def main(api_url, loop_times):
    for i in range(loop_times):
        try:
            download_file(api_url, 'img', i+1)
        except Exception as e:
            print(f"[Warning] Error occurred: {e}")

if __name__ == "__main__":
    api_url = "https://api.dwo.cc/api/ksvideo"
    loop_times = 1000  # Define the number of times to loop
    main(api_url, loop_times)

```

为了处理 API 随机给出链接重复的问题，又让 Copilot 写了个去重脚本，可以多线程执行。

```python
import os
import hashlib
import concurrent.futures

def hash_file(file_path):
    """Generates a hash for a file."""
    hasher = hashlib.md5()
    with open(file_path, 'rb') as file:
        buf = file.read()
        hasher.update(buf)
    return hasher.hexdigest(), file_path

def find_duplicates(directory):
    """Finds duplicate files in the given directory."""
    hashes = {}
    
    with concurrent.futures.ThreadPoolExecutor() as executor:
        # Generate list of files
        file_paths = [
            os.path.join(root, file_name) 
            for root, _, files in os.walk(directory) 
            for file_name in files
        ]
        
        # Process files in parallel
        future_to_file = {executor.submit(hash_file, file_path): file_path for file_path in file_paths}
        
        for future in concurrent.futures.as_completed(future_to_file):
            try:
                file_hash, file_path = future.result()
                if file_hash in hashes:
                    hashes[file_hash].append(file_path)
                else:
                    hashes[file_hash] = [file_path]
            except Exception as e:
                print(f"[Warning] Error processing file: {e}")

    return hashes

def main():
    directory = '/var/www/img/kuaishou/img'  # Replace with the path to your directory
    duplicates = find_duplicates(directory)
    
    if duplicates:
        print("[Info] Duplicate files found and cleaned:")
        for file_hash, file_list in duplicates.items():
            if len(file_list) > 1:
                print(f"[Info] {' | '.join(file_list)}")
                for file_to_remove in file_list[1:]:
                    os.remove(file_to_remove)
                    print(f"[Info] Removed: {file_to_remove}")
    else:
        print("[Info] No duplicate files found.")

if __name__ == "__main__":
    main()

```

这两个脚本总共用了 6 个 Prompt，耗时 15 分钟达到正常运行状态，效果还是相当不错的。Copilot 给出的程序基本都可以直接运行，少数与实际情况非常相关的地方需要手动修改。

> 具体使用的 Prompt 都在上述仓库的代码注释中，为了简明这里就不粘贴了。

## 🌟 秀人网

这里的情况就比较复杂了。虽然秀人网没有设置反盗链，但是没有 API，需要自己设置正则表达式来匹配链接。

但是 Microsoft Copilot 竟然不支持直接访问网页，复制粘贴 HTML 又超出字符限制……只好本蛾子自己来写了，那这个测试就没什么意思，最难的部分都跳过了……

吗？

本蛾子自己写了一版代码，发现根本没法长期执行，一个小小的错误就会让他直接丢 Exception。于是本蛾子将需求输入了 Copilot，并且告诉它了正则表达式，它给出的第一版代码可读性就相当高，并且基本可以运行。后面本蛾子只将调试错误告诉它，经过 6 个 Prompt 的修正，它给出了正确代码。

本蛾子已经执行这段脚本 3 天了，它依然没有出现问题。~~感觉它很快就要替代本蛾子参加秋招了……~~

最后给出的代码，本蛾子稍微手动修改了一点：

```python
import os
import re
import requests
from urllib.parse import urljoin
import warnings

def get_max_dir_num():
    img_dir = 'img'
    if not os.path.exists(img_dir):
        os.makedirs(img_dir)
    
    max_num = 13121
    for dir_name in os.listdir(img_dir):
        try:
            num = int(dir_name)
            if num > max_num:
                max_num = num
        except ValueError:
            pass
    return max_num

def download_images(begin_num):
    url = f'https://www.xiurenwang.cc/{begin_num}.html'
    print(f"[Info] Page URL: {url}")
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }
    response = requests.get(url, headers=headers)
    if response.status_code != 200:
        print(f'[Error] Failed to download page {begin_num}: Code {response.status_code}')
        return

    html = response.text
    pattern = r'(?<=href=")//ooo111\.ka123\.sbs/pic/.*?/.*?/.*?\.jpg'
    matches = re.findall(pattern, html)

    if not matches:
        print(f'[Error] Failed to download page {begin_num}: No images found')
        return

    initial_img_url = urljoin('https:', matches[0])
    initial_img_num = int(initial_img_url.split('/')[-1].split('.')[0])

    img_path = f'img/{begin_num}/{initial_img_num}.jpg'
    os.makedirs(os.path.dirname(img_path), exist_ok=True)

    headers = {
        'Referer': url,
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }

    # Download images with increasing numbers
    for i in range(initial_img_num, initial_img_num + 1200):  # adjust the range as needed
        img_url = initial_img_url.replace(f'{initial_img_num}.jpg', f'{i}.jpg')
        response = requests.get(img_url, headers=headers, verify=False)
        if response.status_code != 200:
            print(f"[Error] Failed to download image {img_url}: Code {response.status_code}")
            print("[Info] Increament terminated")
            break
        img_path = f'img/{begin_num}/{i}.jpg'
        with open(img_path, 'wb') as f:
            f.write(response.content)
        print(f"[Info] Downloaded image {img_url} to {img_path}")
        os.system(f"jpegoptim {img_path} -m40 -q")
        if os.popen(f"identify -format '%w' {img_path}").read() > 1000 or os.popen(f"identify -format '%h' {img_path}").read() > 1000:
            os.system(f"convert -resize 50% {img_path} {img_path}")
        print(f"[Info] Compressed image {img_path}")

    # Download images with decreasing numbers
    for i in range(initial_img_num - 1, 0, -1):  # adjust the range as needed
        img_url = initial_img_url.replace(f'{initial_img_num}.jpg', f'{i}.jpg')
        response = requests.get(img_url, headers=headers, verify=False)
        if response.status_code != 200:
            print(f"[Error] Failed to download image {img_url}: Code {response.status_code}")
            print("[Info] Decreament terminated")
            break
        img_path = f'img/{begin_num}/{i}.jpg'
        with open(img_path, 'wb') as f:
            f.write(response.content)
        print(f"[Info] Downloaded image {img_url} to {img_path}")

def main():
    import sys
    if len(sys.argv) > 1:
        begin_num = int(sys.argv[1])
    else:
        max_dir_num = get_max_dir_num()
        if max_dir_num > 13121:
            begin_num = max_dir_num
        else:
            begin_num = max_dir_num
    if begin_num == 13121:
        print(f"[Info] Use default begin_num {begin_num}")
    elif len(sys.argv) > 1:
        print(f"[Info] Use argument begin_num {begin_num}")
    else:
        print(f"[Info] Use resuming begin_num {begin_num}")

    current_max = 16466
    warnings.filterwarnings("ignore")
    while begin_num <= current_max:
        print(f"[Info] Proceeding page {begin_num}")
        download_images(begin_num)
        begin_num += 1

if __name__ == '__main__':
    main()

```

另外本蛾子还让 Copilot 按要求写了一个展示图片的前端，又改了一个用于上文视频的，HTML 比较长，大家就到仓库里看吧。

## 🛡 困难：次元岛

次元岛的图片链接就不是那么规整了，而且还有混淆项和防盗链，本蛾子在上面秀人网的对话历史上给出 HTML（这次比较短）和要求（防止反盗链、排除部分格式不同的链接），然后要求 Copilot 生成脚本。很可惜，经过 17 个 Prompt 的拉扯，Copilot 并未给出满意的答案，代码却是越来越长、可读性越来越低。

于是，本蛾子在中间选择了一个还不是那么复杂的版本修改了一下，大约改动 30 行，得到了可用代码。具体代码其实和秀人网的很像，但是由于不是 Copilot 生成的，只能说这个挑战是失败的。

详细代码请看仓库。

## 🗣 总结

Copilot 对于原创脚本的处理很好，对于比较简单、条件充足的问题，很好的编程习惯使其代码可用性很强。但是遇到需要修改代码适应新情况的时候，就出现了一些问题。

本蛾子认为 Copilot 虽好，还是不能依赖它，否则就陷入了“易用性”的兔子洞，在遇到问题时无从下手，反而增加麻烦。
